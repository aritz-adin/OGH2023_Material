[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "OpenGeoHub Summer School 2023",
    "section": "",
    "text": "This lecture will provide an introduction to working with geographic data using R in a ‘tidy’ way. It will focus on using the sf package to read, write, manipulate, and plot geographic data in combination with the tidyverse metapackage. Why use the sf package with the tidyverse? The lecture will outline some of the ideas underlying the tidyverse and how they can speed-up data analysis pipelines, while making data analysis code easier to read and write. We will see how the following lines\n\nlibrary(sf)\nlibrary(tidyverse)\n\ncan provide a foundation on which the many geographic data analysis problems can be solved. The lecture will also cover on more recently developed packages that integrate with the tidyverse to a greater and lesser extent. We will look at how the geos package, which provides a simple and high-performance interface to the GEOS library for performing geometric operations on geographic data, integrates with the tidyverse. The tidyverse is not the right tool for every data analysis task and we touch on alternatives for working with raster data, with reference to the terra package, and alternative frameworks such as data.table. Finally, we will also look at how the ‘tidy’ philosophy could be implemented in other programming languages, such as Python.\nThe focus throughout will be on practical skills and using packages effectively within the wider context of project management tools, integrated development environments (we recommend VS Code with appropriate extensions or RStudio), and version control systems.\nMaterial del curso: https://ogh23.robinlovelace.net/\nGitHub repository: https://github.com/robinlovelace/opengeohub2023\nEnlace a la grabación en Youtube: parte 1, parte 2\n\n\n\nA common challenge with raster datasets is not only that they come in large files (single Sentinel-2 tiles are around 1 GB), but that many of these files, potentially thousands or millions, are needed to address the area and time period of interest. In 2022, Copernicus, the program that runs all Sentinel satellites, published 160 TB of images per day. This means that a classic pattern in using R consisting of downloading data to local disc, loading the data in memory, and analysing it is not going to work. This lectures describes how large spatial and spatiotemporal datasets can be handled with R, with a focus on packages sf and stars.\nFor practical use, we classify large datasets as too large:\n\nto fit in working memory,\nto fit on the local hard drive, or\nto download to locally managed infrastructure (such as network attached storage)\n\nThese three categories may (today) correspond very roughly to Gigabyte-, Terabyte- and Petabyte-sized datasets. Besides size considerations, access and processing speed also play a role, in particular for larger datasets or interactive applications. Cloud native geospatial formats are formats optimised with processing on cloud infrastructure in mind, where costs of computing and storage need to be considered and optimised.\nMaterial del curso: https://edzer.github.io/OGH23/dc.html / R book\nGitHub repository: https://github.com/edzer/OGH23\nEnlace a la grabación en Youtube: parte 1, parte 2\n\n\n\nUnsupervised classification of satellite images is the process of grouping similar pixels on an image into homogeneous clusters based primarily on their spectral characteristics. This approach does not require reference (labeled) data, unlike supervised classification, therefore it can be used as a method of first choice. Satellite image classification is commonly used in a variety of fields, including environmental monitoring, land cover mapping, and disaster management. The generated thematic maps can be used to identify and monitor changes in land use, and assess the impact of natural disasters.\nDuring this workshop, participants will gain practical knowledge and skills to perform unsupervised classification of Landsat data using the R language. It will be demonstrated step by step how to use and prepare raster data for analysis, popular grouping methods will be discussed and finally we will prepare a land cover map with interpretation of the results. The workshop will also cover the challenges and limitations of unsupervised classification, such as subjective interpretation of results difficulty of selecting the optimal number of clusters, and validation methods for ensuring the accuracy and reliability of results.\nThe workshop is aimed at beginners, but basic knowledge of GIS and satellite remote sensing is required.\nMaterial del curso: https://kadyb.github.io/OGH2023/\nGitHub repository: https://github.com/kadyb/OGH2023\nEnlace a la grabación en Youtube: parte 1, parte 2\n\n\n\nPython is an extremely popular general-purpose programming language. It is used in a wide range of settings and for various purposes, including for spatial data processing and analysis.\nThe aim of this tutorial is to give an introduction to methods of working with spatial data using Python. The tutorial will be split into two parts, introducing two central Python packages:\n\ngeopandas---For working with vector layers\nrasterio---For working with rasters\n\nThe tutorial will demonstrate typical basic workflows of processing spatial data: data input, processing, geo-computation, and exporting of the results. We will use realistic datasets, such as GTFS public transport data and remote sensing products.\nBy the end of the tutorial, the participants will be able to:\n\nImport spatial data from files\nSubset and process the data\nGraphically display the data\nPerform spatial calculations (such as calculating distances, or applying raster algebra operators)\nExport the results\n\nTo follow along and reproduce the results on your own computer, the prerequisite is to be able to run Python code in a Jupyter Notebook interface, linked to a Python environment with the two above-mentioned packages installed. Instructions will be sent in advance.\nMaterial del curso: https://geobgu.xyz/presentations/p_2023_ogh/\n\n\n\nJulia is a programming language that is simple to write and scriptable like Python and R, but fast like C or C++. At 10 years, it's a young language, so the ecosystem isn't as large and mature as you want it to be. Maarten Pronk was an early adopter of the language in his research at Deltares, a Dutch research institute. In this lecture(s) he will introduce Julia, his motivation to use it and his OSS journey. Half of the lecture will be non-spatial, while the latter half will focus on the JuliaGeo ecosystem and showcased some of the possibilities of the Julia language.\n\nThe JuliaGeo GitHub organization is intended primarily for the collaborative development of packages that are generally applicable across the geospatial and geosciences domains. For dealing with geospatial data, packages from the JuliaGeometry and JuliaImages organizations may also be of interest, and we will aim for good integration with those. Since the JuliaGeo organization aims to provide mostly general tools, more domain specific packages may be better suited for development in domain specific organizations. JuliaClimate is a nice example of such an organization that will be especially interesting to climate, atmosphere and ocean scientists. EcoJulia also provides some tools for generating and downloading spatial data sets, with a focus on ecological applications.\nGitHub repository: https://github.com/evetion/OGH2023"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\nThe content below was rendered from README.qmd in the aritz-adin/OGH2023_Material GitHub repository where you can find the source code for this website."
  },
  {
    "objectID": "index.html#opengeohub-foundation",
    "href": "index.html#opengeohub-foundation",
    "title": "OpenGeoHub Summer School 2023",
    "section": "",
    "text": "Enlaces de interés:\n\nOpenGeoHub video portal (IB AV-Portal)\nYoutube channel\n\nLibros online sobre Geocomputation con R, Python y Julia:\n\nR books: Geocomputation with R, Spatial Data Science, Geographic Data Science with R\nPython books: Geocomputation with Python, Introduction to Python for Geographic Data Analysis, Geographic Data Science with Python\nJulia video: JuliaGeo: a gentle introduction"
  },
  {
    "objectID": "index.html#summer-school-2023-processing-and-visualizing-large-geospatial-data-using-r-python-and-julia",
    "href": "index.html#summer-school-2023-processing-and-visualizing-large-geospatial-data-using-r-python-and-julia",
    "title": "OpenGeoHub Summer School 2023",
    "section": "Summer School 2023: “Processing and visualizing large geospatial data using R, Python and Julia”",
    "text": "Summer School 2023: “Processing and visualizing large geospatial data using R, Python and Julia”\n\n\nTidy geographic data with sf, dplyr, ggplot2, geos and friends (Robin Lovelace)\n\nAbstract:\nThis lecture will provide an introduction to working with geographic data using R in a ‘tidy’ way. It will focus on using the sf package to read, write, manipulate, and plot geographic data in combination with the tidyverse metapackage. Why use the sf package with the tidyverse? The lecture will outline some of the ideas underlying the tidyverse and how they can speed-up data analysis pipelines, while making data analysis code easier to read and write. We will see how the following lines\n\nlibrary(sf)\nlibrary(tidyverse)\n\ncan provide a foundation on which the many geographic data analysis problems can be solved. The lecture will also cover on more recently developed packages that integrate with the tidyverse to a greater and lesser extent. We will look at how the geos package, which provides a simple and high-performance interface to the GEOS library for performing geometric operations on geographic data, integrates with the tidyverse. The tidyverse is not the right tool for every data analysis task and we touch on alternatives for working with raster data, with reference to the terra package, and alternative frameworks such as data.table. Finally, we will also look at how the ‘tidy’ philosophy could be implemented in other programming languages, such as Python.\nThe focus throughout will be on practical skills and using packages effectively within the wider context of project management tools, integrated development environments (we recommend VS Code with appropriate extensions or RStudio), and version control systems.\nMaterial del curso: https://ogh23.robinlovelace.net/\nGitHub repository: https://github.com/robinlovelace/opengeohub2023\nEnlace a la grabación en Youtube: parte 1, parte 2\n\n\n\nRaster and vector data cubes in R (Edzer Pebesma)\n\nAbstract:\nA common challenge with raster datasets is not only that they come in large files (single Sentinel-2 tiles are around 1 GB), but that many of these files, potentially thousands or millions, are needed to address the area and time period of interest. In 2022, Copernicus, the program that runs all Sentinel satellites, published 160 TB of images per day. This means that a classic pattern in using R consisting of downloading data to local disc, loading the data in memory, and analysing it is not going to work. This lectures describes how large spatial and spatiotemporal datasets can be handled with R, with a focus on packages sf and stars.\nFor practical use, we classify large datasets as too large:\n\nto fit in working memory,\nto fit on the local hard drive, or\nto download to locally managed infrastructure (such as network attached storage)\n\nThese three categories may (today) correspond very roughly to Gigabyte-, Terabyte- and Petabyte-sized datasets. Besides size considerations, access and processing speed also play a role, in particular for larger datasets or interactive applications. Cloud native geospatial formats are formats optimised with processing on cloud infrastructure in mind, where costs of computing and storage need to be considered and optimised.\nMaterial del curso: https://edzer.github.io/OGH23/dc.html / R book\n\n\n\nGitHub repository: https://github.com/edzer/OGH23\nEnlace a la grabación en Youtube: parte 1, parte 2"
  },
  {
    "objectID": "index.html#tidy-geographic-data-with-sf-dplyr-ggplot2-geos-and-friends-robin-lovelace",
    "href": "index.html#tidy-geographic-data-with-sf-dplyr-ggplot2-geos-and-friends-robin-lovelace",
    "title": "OpenGeoHub Summer School 2023",
    "section": "",
    "text": "This lecture will provide an introduction to working with geographic data using R in a ‘tidy’ way. It will focus on using the sf package to read, write, manipulate, and plot geographic data in combination with the tidyverse metapackage. Why use the sf package with the tidyverse? The lecture will outline some of the ideas underlying the tidyverse and how they can speed-up data analysis pipelines, while making data analysis code easier to read and write. We will see how the following lines\n\nlibrary(sf)\nlibrary(tidyverse)\n\ncan provide a foundation on which the many geographic data analysis problems can be solved. The lecture will also cover on more recently developed packages that integrate with the tidyverse to a greater and lesser extent. We will look at how the geos package, which provides a simple and high-performance interface to the GEOS library for performing geometric operations on geographic data, integrates with the tidyverse. The tidyverse is not the right tool for every data analysis task and we touch on alternatives for working with raster data, with reference to the terra package, and alternative frameworks such as data.table. Finally, we will also look at how the ‘tidy’ philosophy could be implemented in other programming languages, such as Python.\nThe focus throughout will be on practical skills and using packages effectively within the wider context of project management tools, integrated development environments (we recommend VS Code with appropriate extensions or RStudio), and version control systems.\nMaterial del curso: https://ogh23.robinlovelace.net/\nGitHub repository: https://github.com/robinlovelace/opengeohub2023\nEnlace a la grabación en Youtube: parte 1, parte 2"
  },
  {
    "objectID": "index.html#raster-and-vector-data-cubes-in-r-edzer-pebesma",
    "href": "index.html#raster-and-vector-data-cubes-in-r-edzer-pebesma",
    "title": "OpenGeoHub Summer School 2023",
    "section": "",
    "text": "A common challenge with raster datasets is not only that they come in large files (single Sentinel-2 tiles are around 1 GB), but that many of these files, potentially thousands or millions, are needed to address the area and time period of interest. In 2022, Copernicus, the program that runs all Sentinel satellites, published 160 TB of images per day. This means that a classic pattern in using R consisting of downloading data to local disc, loading the data in memory, and analysing it is not going to work. This lectures describes how large spatial and spatiotemporal datasets can be handled with R, with a focus on packages sf and stars.\nFor practical use, we classify large datasets as too large:\n\nto fit in working memory,\nto fit on the local hard drive, or\nto download to locally managed infrastructure (such as network attached storage)\n\nThese three categories may (today) correspond very roughly to Gigabyte-, Terabyte- and Petabyte-sized datasets. Besides size considerations, access and processing speed also play a role, in particular for larger datasets or interactive applications. Cloud native geospatial formats are formats optimised with processing on cloud infrastructure in mind, where costs of computing and storage need to be considered and optimised.\nMaterial del curso: https://edzer.github.io/OGH23/dc.html / R book\nGitHub repository: https://github.com/edzer/OGH23\nEnlace a la grabación en Youtube: parte 1, parte 2"
  },
  {
    "objectID": "OGH_Foundation.html",
    "href": "OGH_Foundation.html",
    "title": "OpenGeoHub Foundation",
    "section": "",
    "text": "Enlaces de interés:\n\nhttps://opengeohub.org\nOpenGeoHub video portal (IB AV-Portal)\nYoutube channel\n\nLibros online sobre Geocomputation con R, Python y Julia:\n\nR books: Geocomputation with R, Spatial Data Science, Geographic Data Science with R\nPython books: Geocomputation with Python, Introduction to Python for Geographic Data Analysis, Geographic Data Science with Python\nJulia video: JuliaGeo: a gentle introduction"
  },
  {
    "objectID": "OGH_Foundation.html#opengeohub-foundation",
    "href": "OGH_Foundation.html#opengeohub-foundation",
    "title": "OpenGeoHub Foundation",
    "section": "",
    "text": "Enlaces de inter�s:\n\nOpenGeoHub video portal (IB AV-Portal)\nYoutube channel\n\nLibros online sobre Geocomputation con R, Python y Julia:\n\nR books: Geocomputation with R, Spatial Data Science, Geographic Data Science with R\nPython books: Geocomputation with Python, Introduction to Python for Geographic Data Analysis, Geographic Data Science with Python\nJulia video: JuliaGeo: a gentle introduction"
  },
  {
    "objectID": "index.html#unsupervised-classification-clustering-of-satellite-images-with-r-krzysztof-dyba",
    "href": "index.html#unsupervised-classification-clustering-of-satellite-images-with-r-krzysztof-dyba",
    "title": "OpenGeoHub Summer School 2023",
    "section": "",
    "text": "Unsupervised classification of satellite images is the process of grouping similar pixels on an image into homogeneous clusters based primarily on their spectral characteristics. This approach does not require reference (labeled) data, unlike supervised classification, therefore it can be used as a method of first choice. Satellite image classification is commonly used in a variety of fields, including environmental monitoring, land cover mapping, and disaster management. The generated thematic maps can be used to identify and monitor changes in land use, and assess the impact of natural disasters.\nDuring this workshop, participants will gain practical knowledge and skills to perform unsupervised classification of Landsat data using the R language. It will be demonstrated step by step how to use and prepare raster data for analysis, popular grouping methods will be discussed and finally we will prepare a land cover map with interpretation of the results. The workshop will also cover the challenges and limitations of unsupervised classification, such as subjective interpretation of results difficulty of selecting the optimal number of clusters, and validation methods for ensuring the accuracy and reliability of results.\nThe workshop is aimed at beginners, but basic knowledge of GIS and satellite remote sensing is required.\nMaterial del curso: https://kadyb.github.io/OGH2023/\nGitHub repository: https://github.com/kadyb/OGH2023\nEnlace a la grabación en Youtube: parte 1, parte 2"
  },
  {
    "objectID": "index.html#introduction-to-working-with-spatial-data-in-python-michael-dorman",
    "href": "index.html#introduction-to-working-with-spatial-data-in-python-michael-dorman",
    "title": "OpenGeoHub Summer School 2023",
    "section": "",
    "text": "Python is an extremely popular general-purpose programming language. It is used in a wide range of settings and for various purposes, including for spatial data processing and analysis.\nThe aim of this tutorial is to give an introduction to methods of working with spatial data using Python. The tutorial will be split into two parts, introducing two central Python packages:\n\ngeopandas---For working with vector layers\nrasterio---For working with rasters\n\nThe tutorial will demonstrate typical basic workflows of processing spatial data: data input, processing, geo-computation, and exporting of the results. We will use realistic datasets, such as GTFS public transport data and remote sensing products.\nBy the end of the tutorial, the participants will be able to:\n\nImport spatial data from files\nSubset and process the data\nGraphically display the data\nPerform spatial calculations (such as calculating distances, or applying raster algebra operators)\nExport the results\n\nTo follow along and reproduce the results on your own computer, the prerequisite is to be able to run Python code in a Jupyter Notebook interface, linked to a Python environment with the two above-mentioned packages installed. Instructions will be sent in advance.\nMaterial del curso: https://geobgu.xyz/presentations/p_2023_ogh/"
  },
  {
    "objectID": "index.html#processing-geospatial-data-using-juliageo-framework-marteen-pronk",
    "href": "index.html#processing-geospatial-data-using-juliageo-framework-marteen-pronk",
    "title": "OpenGeoHub Summer School 2023",
    "section": "",
    "text": "Julia is a programming language that is simple to write and scriptable like Python and R, but fast like C or C++. At 10 years, it's a young language, so the ecosystem isn't as large and mature as you want it to be. Maarten Pronk was an early adopter of the language in his research at Deltares, a Dutch research institute. In this lecture(s) he will introduce Julia, his motivation to use it and his OSS journey. Half of the lecture will be non-spatial, while the latter half will focus on the JuliaGeo ecosystem and showcased some of the possibilities of the Julia language.\n\nThe JuliaGeo GitHub organization is intended primarily for the collaborative development of packages that are generally applicable across the geospatial and geosciences domains. For dealing with geospatial data, packages from the JuliaGeometry and JuliaImages organizations may also be of interest, and we will aim for good integration with those. Since the JuliaGeo organization aims to provide mostly general tools, more domain specific packages may be better suited for development in domain specific organizations. JuliaClimate is a nice example of such an organization that will be especially interesting to climate, atmosphere and ocean scientists. EcoJulia also provides some tools for generating and downloading spatial data sets, with a focus on ecological applications.\nGitHub repository: https://github.com/evetion/OGH2023"
  }
]